<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kafka on Raku Programming</title>
    <link>https://ohmysummer.github.io/tags/kafka/</link>
    <description>Recent content in Kafka on Raku Programming</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 20 Jul 2018 20:14:28 +0800</lastBuildDate>
    
	<atom:link href="https://ohmysummer.github.io/tags/kafka/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>使用 Raku 连接 Kafka</title>
      <link>https://ohmysummer.github.io/post/2018-07-20-%E4%BD%BF%E7%94%A8raku%E8%BF%9E%E6%8E%A5kafka/</link>
      <pubDate>Fri, 20 Jul 2018 20:14:28 +0800</pubDate>
      
      <guid>https://ohmysummer.github.io/post/2018-07-20-%E4%BD%BF%E7%94%A8raku%E8%BF%9E%E6%8E%A5kafka/</guid>
      <description>有这样一个场景, 数据发送方将压缩文件读成字节数组后发往 Kafka, 然后第三方的 Kafka Client 从中读取字节数组解压缩, 每条 message 对应一个压缩文件, 每个压缩文件中包含 _log.txt 和 _result.txt。
Raku 可以从 Kafka 中读取消息并完成解析。
首先安装相关模块: Pkafka 用于和 Kafka 交互； Archive::Libarchive 用于解压缩字节数组。 Cro 用于 HTTP 请求，DBiish 用于数据库读写。
zef install Pkafka zef install Archive::Libarchive zef install Cro zef install DBIish  代码片段如下:
use PKafka::Consumer; use PKafka::Message; use PKafka::Producer; use Archive::Libarchive; use Archive::Libarchive::Constants; use Cro::HTTP::Client; use JSON::Fast; use JSON::Path; use DBIish; sub MAIN () { my $brokers = &amp;quot;127.0.0.1&amp;quot;; my $test = PKafka::Consumer.</description>
    </item>
    
  </channel>
</rss>